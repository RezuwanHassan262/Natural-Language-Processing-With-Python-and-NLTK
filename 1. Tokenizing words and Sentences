{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Tokenizing words and Sentences","provenance":[],"authorship_tag":"ABX9TyPynTwriyU4zhC6bceiDIKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n","\n","* Word tokenizer = Separates by words\n","\n","* Sentence tokenizer = Separates by sentence\n","\n","Two popular terms are, CORPUS & LEXICON\n","\n","Corpus : Basically, A body of texts\n","\n","Lexicon : Basically, A dictionary of woords and their meaning (According to all context)"],"metadata":{"id":"j4e7rNSFTxAR"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-tGFYPRGnwe","executionInfo":{"status":"ok","timestamp":1649021810540,"user_tz":-360,"elapsed":402,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"416600e1-9232-414a-aebb-09ae25c25a84"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","\n","\n"," Sentence tokenizer: ['Hello Mr. Smith, how are you doing today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard.'] \n","\n","\n","\n"," Word tokenizer: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.'] \n","\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","example_text = \"Hello Mr. Smith, how are you doing today? The weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard.\"\n","\n","print()\n","print('\\n',\"Sentence tokenizer:\",sent_tokenize(example_text),'\\n')\n","print()\n","print('\\n',\"Word tokenizer:\",word_tokenize(example_text),'\\n')"]},{"cell_type":"markdown","source":["Keeping count of the words"],"metadata":{"id":"a_mj2Eb5YtnE"}},{"cell_type":"code","source":["d = {}\n","for i in word_tokenize(example_text):\n","  if i not in d:\n","    d[i] = 1\n","  else:\n","    d[i] = d[i]+ 1\n","print(d)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txTBAmeLRvEv","executionInfo":{"status":"ok","timestamp":1649022246421,"user_tz":-360,"elapsed":16,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"36065947-7c23-4f3d-955f-f5ed78ffb6c9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Hello': 1, 'Mr.': 1, 'Smith': 1, ',': 1, 'how': 1, 'are': 1, 'you': 1, 'doing': 1, 'today': 1, '?': 1, 'The': 2, 'weather': 1, 'is': 3, 'great': 1, 'and': 1, 'Python': 1, 'awesome': 1, '.': 3, 'sky': 1, 'pinkish-blue': 1, 'You': 1, 'should': 1, 'not': 1, 'eat': 1, 'cardboard': 1}\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"e7ME1tkeYpml"},"execution_count":null,"outputs":[]}]}